{"cells":[{"cell_type":"markdown","source":["# Refreshing and Loading a table from a Power BI Semantic Model into a Fabric Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0144677b-6d59-4309-a202-75e3f28e9024"},{"cell_type":"markdown","source":["##### Been having a ton of fun learning fabric and playing with pySpark. This notebook will:\n","##### - trigger a PBI semantic model refresh.\n","##### - wait until the refresh has completed.\n","##### - load a table from the newly refreshed semantic model into a lakehouse table. \n","\n","##### This allows you to schedule a fabric notebook to refresh:\n","#####   - a PBI calculated table.\n","#####   - a bunch of PBI reports cross multiple workspaces in and run code once they have all finished."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"19341706-8206-4f16-ad78-d2e45d2a8112"},{"cell_type":"markdown","source":["### Step 1: Import Modules\n","###### Once you import a module in python you can use the functions and variables within the module in within your code."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b28b680a-478a-4ee9-a944-208b1faad29f"},{"cell_type":"markdown","source":["Install the SemPy Python library in the notebook kernel and import the SemPy fabric module as \"fabric.\" We will be using this package to interact with the Power BI semantic model we want to load data from. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"77b195a7-7b18-4122-9eed-856a69fd18db"},{"cell_type":"code","source":["%pip install semantic-link\n","import sempy.fabric as fabric"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}}},"id":"275b5d6f-f9cf-4081-9740-93c53223e292"},{"cell_type":"markdown","source":["Import the functions module from the PySpark SQL package. The PySpark.SQL package is provides functions to work with structured data using spark. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"23e3e974-1556-4397-b20e-1a277060d79e"},{"cell_type":"code","source":["import pyspark.sql.functions as f"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2a50d3fd-fecd-4b25-9583-96a864a17664"},{"cell_type":"markdown","source":["Import the time package; we will be using the time.Sleep() function to pause the notebook's execution as we wait for the Power BI table we want to load to finish refreshing."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1cab52c9-5105-4234-b877-948f2d6ca85f"},{"cell_type":"code","source":["import time"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}}},"id":"7e266cc9-05fb-49f5-9bfd-fd5e68725bac"},{"cell_type":"markdown","source":["Import the re package; we will be using this to implement a regex expression to allow us to ensure the Power BI table column names comply with the Fabric Lakehouse's column name requirements."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b7fa0a92-95c1-4265-ada8-6967d9c4756d"},{"cell_type":"code","source":["import re"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"368e7a87-bdfc-44a2-845d-9bd5484d24aa"},{"cell_type":"markdown","source":["### Step 2: Setting Variables\n","###### We are setting variables with the:\n","- Workspace name the Power BI semantic model we want the data from exists within.\n","- Semantic model name the table we want the data from.\n","- Power BI Table name within the Semantic model we want data from.\n","- Name of the table within the Fabric Lakehouse we want to create or overwrite. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f94bf2b9-b6f5-4745-9bb8-1d506cfe9563"},{"cell_type":"code","source":["#Setting the workspace name\n","workspace = 'Sneaker Workspace'\n","#Setting the semantic model name\n","semantic_model = 'Nike Sneaker Calendar'\n","#Setting the PBI Table name\n","pbi_source_table = 'Launches'\n","#Setting the Lakehouse Table name\n","lakehouse_destination_table = 'launches_export'"],"outputs":[],"execution_count":null,"metadata":{},"id":"29fff044-f888-4951-9bfa-e96d733c2002"},{"cell_type":"markdown","source":["### Step 3: Refreshing the Power bi Table\n","###### Before we load data into the Fabric Lakehouse we need to make sure it is refreshed\n","We are going to request a refresh of the Power BI table we want to load into the Fabric lakehouse and then use a while loop to check if the dataset is refreshed. We will break out of the loop if:\n","- Our requested refresh has a status of 'Completed'\n","- Our requested refresh has a status of 'Failed'\n","- The loop has been 120% of the average refresh time of the semantic model\n","\n","If we are going to be scheduling this notebook we want to avoid a never ending loop, and the above conditions will help us with that. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ce38c8f8-bdd0-4300-ad3a-996efb9903ec"},{"cell_type":"markdown","source":["We use the code below to determine the average refresh duration of the model. Subsequently, we divide that number by 5 to calculate 20% of the average refresh duration."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a490a72b-c622-4535-8e35-4843a50ade80"},{"cell_type":"code","source":["#getting a pandas dataframe of all the refreshes of the model we are interested in refreshing the table in using the fabric API\n","refresh_history = fabric.list_refresh_requests(workspace=workspace, dataset=semantic_model)\n","#Converting the pandas dataframe into a spark dataframe\n","refresh_history_spark = spark.createDataFrame(refresh_history)\n","#adding a calculated column to represent the refresh duration of each run\n","refresh_history_spark = refresh_history_spark.withColumn( \\\n","        'Refresh Duration', \\\n","        (f.col('End Time').cast('long') - f.col('Start Time').cast('long'))\n","    )\n","#getting the average duration using the newly created calculated column\n","refresh_average_duration = refresh_history_spark.agg({'Refresh Duration': 'avg'}).first()[0]\n","#Calculating 20% of the average refresh time by dividing by five.\n","time_to_sleep = refresh_average_duration / 5"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"dba28bba-268f-4d59-b9c1-0e9d1c10c2f9"},{"cell_type":"markdown","source":["We will now put the table we want to refresh into a dictionary and then submit it for a refresh"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"80659706-e981-459f-9920-c9b11d2020e2"},{"cell_type":"code","source":["#Create a dictionary with the specific table we want to refresh\n","table_dictionary = [{'table': pbi_source_table}]\n","#Use the fabric api to request a refresh of the table using the new dictionary. \n","#This will return a refresh request ID which we are stroing in a variable.\n","refresh_request_id = fabric.refresh_dataset(workspace=workspace, \\\n","                                            dataset=semantic_model, \\\n","                                            objects=table_dictionary)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a1234e84-19d5-4f1a-98bd-e8986584f357"},{"cell_type":"markdown","source":["We will now use a loop to check if the refresh of table has completed. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3c76e5de-2e87-4545-9901-8c640ab601a1"},{"cell_type":"code","source":["#setting a counter to ensure the loop does not run more then 6 times\n","loop_count = 0\n","#creating a variable outside of the loop to store the refresh status in\n","refresh_status = ''\n","#storing the refresh statuses that will break the loop \n","exit_statuses = ['Completed','Failed']\n","#Creating a while loop\n","while loop_count < 6:\n","    #Gets the current status of the power bi refresh\n","    refresh_status = fabric.get_refresh_execution_details(workspace=workspace,dataset=semantic_model,refresh_request_id=refresh_request_id).status\n","    #if the refresh status is within the exit status array exiting the loop\n","    if refresh_status in exit_statuses:\n","        break\n","    #Sleeping the code for 20% of the average refresh time\n","    time.sleep(time_to_sleep)\n","    #adding 1 to the loop count so the loop will not run endlessly \n","    ++loop_count"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"4ce21fdc-5bda-43f7-8c9c-c8f660eb67ef"},{"cell_type":"markdown","source":["Then if the table has refreshed the below code will now load the table into the Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"05d30e9d-b082-46c5-8211-d02c743d1e0c"},{"cell_type":"code","source":["#checking to see if the refresh has completed\n","if refresh_status == 'Completed':\n","    #if the refresh has completed reading the table into a dataframe\n","    table_fabric_df = fabric.read_table(workspace=workspace, dataset=semantic_model, table=pbi_source_table)\n","    #changing the table into a spark data frame\n","    table_spark_df = spark.createDataFrame(table_fabric_df)\n","    #removing things like spaces from the column names of the data frame\n","    table_spark_df = table_spark_df.select([f.col(column_name).alias(re.sub('[^0-9a-zA-Z$]+', '', column_name)) for column_name in table_spark_df.columns])\n","    #loading the table into the lakehouse \n","    #note -- I am using a overwrite, write mode you could use an append\n","    table_spark_df.write.mode('overwrite').option('overwriteSchema', 'true').format('delta').saveAsTable(lakehouse_destination_table)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d46ceeb5-5a18-4a50-b368-97b1c4ad69cb"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"known_lakehouses":[{"id":"6ee11adb-43f3-4236-acbf-b50e9f875eb6"}],"default_lakehouse":"6ee11adb-43f3-4236-acbf-b50e9f875eb6","default_lakehouse_name":"Lakehouse_Demo","default_lakehouse_workspace_id":"bc32996a-128f-455d-85a9-50a2a02c3142"}}},"nbformat":4,"nbformat_minor":5}