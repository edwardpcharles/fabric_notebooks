{"cells":[{"cell_type":"markdown","source":["# The purpose of this notebook is to load business plans from an Excel file into a existing budget planning table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e0869acd-a982-431d-babc-ba5ce39a6f96"},{"cell_type":"markdown","source":["Step 1 - Importing Python packages needed"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ca9d791-4792-4773-ad05-883b9f3004d3"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import uuid\n","from datetime import datetime\n","from pyspark.sql.types import *"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9770985Z","session_start_time":"2025-02-10T02:00:24.9784854Z","execution_start_time":"2025-02-10T02:00:40.8796387Z","execution_finish_time":"2025-02-10T02:00:43.739009Z","parent_msg_id":"cac2085c-8611-42dc-aa6f-04dc43c63d96"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9e83d364-f613-48a5-8b9f-89ff67ece3ac"},{"cell_type":"markdown","source":["Step 2 - Importing the excel file into Pandas data frame. Interesting piece of code being used in this cell:\n","\n","pandas.melt() is a function in the pandas library that \"unpivots\" or \"melts\" a DataFrame from a wide format to a long format.  Think of it like taking columns and stacking them into rows.  This is incredibly useful for preparing data for analysis or visualization, especially when working with data that has multiple measurements or variables associated with a single observation."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e28890b-35a6-493f-97be-c87c6158ccb3"},{"cell_type":"code","source":["#path to excel file with targets \n","excel_path = \"./builtin/fake_budget_update1.xlsx\"\n","#excel sheet name with targets\n","sheet_nam = \"Sheet1\"\n","#reading the excel file into pandas data frame\n","df = pd.read_excel(excel_path, sheet_name=sheet_nam)\n","\n","#looping through all the values in data frame and replacing white space with null\n","for col in df.columns:\n","    df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True)\n","#dropping all rows where all values are null\n","df = df.dropna(how=\"all\")\n","#transposing all the columns except for the ones in the array id_vars is set to\n","df = pd.melt(df, id_vars=[\"Plan Version\", \"Model\", \"Metric\"], value_vars=None, var_name=\"Time Period\", value_name=\"value\")\n","#putting all columns in the data frame into cammel case\n","df.columns = ['_'.join(col.strip().split()).lower() for col in df.columns]\n","#converts time period to string\n","df['time_period'] = df['time_period'].astype(str)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9780564Z","session_start_time":null,"execution_start_time":"2025-02-10T02:00:43.9604977Z","execution_finish_time":"2025-02-10T02:00:46.4247178Z","parent_msg_id":"981815b1-0334-4c5b-a734-091200d46f64"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"15de0429-654c-4db4-8f4d-bd143c08d8b1"},{"cell_type":"markdown","source":["Step 3 - Adding Columns to facilitate the type 2 table: Record Start, Record End, Transaction Type, Active Record Flag, Unique ID. Interesting Piece of code being used in this cell:\n","\n","uuid.UUID5 - uuid5 takes the namespace UUID and the name string as input. It then combines them and uses the SHA-1 hashing algorithm to produce a 128-bit UUID. Crucially, the same namespace and name will always produce the same UUID. This is the key benefit of version 5 UUIDs.\n","\n","Why use uuid5?\n","\n","- Reproducibility: If you have a name and a namespace, you can reliably generate the same UUID every time. This is useful for creating consistent identifiers for resources, especially when you need to refer to the same resource across different systems or databases.\n","- Uniqueness within a namespace: While not guaranteed to be globally unique like version 4 UUIDs (randomly generated), version 5 UUIDs provide uniqueness within the context of a given namespace. This is often sufficient and avoids the storage overhead of truly random UUIDs when you have a well-defined namespace.\n","- Avoiding collisions: By using a namespace, you reduce the risk of collisions (two different names generating the same UUID) compared to just hashing the name directly.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2b2315d0-48b4-473e-b4b7-09ad3e88119a"},{"cell_type":"code","source":["#adding additional columns \n","df['record_valid_start'] = pd.to_datetime('1900-01-01')\n","df['record_valid_end'] = pd.Timestamp.max\n","df['transaction_typ'] = 'I'\n","df['active_record_flag'] = True\n","#calculating a unique id by using uuid5 \n","namespace = namespace = uuid.UUID('1f925fee-159c-44e3-b9d0-aadbf58cf7ff')\n","df['id'] = df.apply(lambda row: str(uuid.uuid5(namespace, '-'.join([str(row['plan_version']), str(row['model']), str(row['metric']), str(row['time_period'])]))), axis=1)\n","#moving the id column to the front of the data frame\n","df = df[['id'] + [col for col in df.columns if col != 'id']]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9788398Z","session_start_time":null,"execution_start_time":"2025-02-10T02:00:46.5528078Z","execution_finish_time":"2025-02-10T02:00:46.8524927Z","parent_msg_id":"df309e9a-7326-4f4d-9224-1639871b6da8"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e3b1d03a-dcb5-4ed9-8718-4e4acf274579"},{"cell_type":"markdown","source":["Step 4 - Converting the Pandas data frame to a spark data frame. You define a StructType for a Spark DataFrame for several important reasons, all related to ensuring data quality, consistency, and efficient processing:\n","\n","- Schema Enforcement:  A StructType defines the schema of your DataFrame.  This includes the names of the columns, their data types (e.g., String, Integer, Double, Date), and whether they can be null.  By explicitly defining the schema, you enforce it when reading or creating the DataFrame.  This helps prevent data inconsistencies and errors.  Spark can then validate the data against the schema.  If data doesn't conform, Spark can either reject it or handle it according to your configuration (e.g., by filling in default values).\n","- Data Type Safety:  Specifying data types in the StructType ensures that Spark treats the data correctly. For example, if you intend to perform numerical calculations on a column, you must define it as a numeric type (Integer, Double, etc.).  Without a schema, Spark might infer the wrong data type (e.g., String) and cause errors when you try to perform calculations.\n","- Performance Optimization: When Spark knows the data types of your columns upfront (thanks to the StructType), it can optimize query execution.  It can choose more efficient data storage formats and processing strategies.  Without a schema, Spark has to infer the data types, which can be slower.\n","- Metadata Management: The StructType provides metadata about your data, which can be very useful for data governance and documentation.  It makes it clear what each column represents and what type of data it should contain.\n","- Interoperability:  Defining a schema makes it easier to exchange data between different systems.  If you have a well-defined schema, other systems can understand the structure and data types of your Spark DataFrame.\n","- Data Validation:  As mentioned earlier, you can use the schema for data validation. You can specify constraints within the schema, such as allowing nulls or setting maximum/minimum values. This way, you catch data quality issues early on.\n","- Working with Nested Data: StructType is essential for working with nested data structures.  You can define complex schemas that include nested structs, arrays, and maps. This is crucial for handling semi-structured data like JSON.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9cce2812-61e7-41d6-a4ad-da70be45bbe3"},{"cell_type":"code","source":["#setting up a spark data frame so spark does not incorrectly infer the schma\n","schema = StructType([\n","    StructField(\"id\", StringType(), False),\n","    StructField(\"plan_version\", StringType(), False),\n","    StructField(\"model\", StringType(), False),\n","    StructField(\"metric\", StringType(), False),\n","    StructField(\"time_period\", StringType(), False),\n","    StructField(\"value\", DoubleType(), True),\n","    StructField(\"record_valid_start\", DateType(), False),\n","    StructField(\"record_valid_end\", DateType(), False),\n","    StructField(\"transaction_typ\", StringType(), False),\n","    StructField(\"active_record_flg\", BooleanType(), False) \n","])\n","#converting the pandas data frame to a spark data frame\n","spark_df = spark.createDataFrame(df, schema=schema)\n","#create temporary view with the records that we want to merge in to the existing table\n","spark_df.createOrReplaceTempView(\"targets_update\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9796147Z","session_start_time":null,"execution_start_time":"2025-02-10T02:00:46.9778282Z","execution_finish_time":"2025-02-10T02:00:48.4931573Z","parent_msg_id":"8f273aff-9b3f-4e59-a5f0-4bb4a1774775"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"5feb4995-6637-47b3-a993-e7509c95d763"},{"cell_type":"markdown","source":["Step 5 - Storing the current date and time when the notebook runs allows us to consistently record the start time for the Type 2 table records."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"715e60bc-c081-4e74-9098-f1d08236dd57"},{"cell_type":"code","source":["#storing the date time \n","record_update_time= datetime.now()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.980357Z","session_start_time":null,"execution_start_time":"2025-02-10T02:00:48.6244728Z","execution_finish_time":"2025-02-10T02:00:48.8670018Z","parent_msg_id":"f26e9e89-0eb8-44e1-850c-f63a2f96dea0"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eef92643-6d4a-4b4b-a3f5-911d7d6d30d0"},{"cell_type":"markdown","source":["Step 6 - defining a SQL Query to identify records that are either new or updates to existing records. Using string interpolation to insert the record start date time variable set in step 5"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7d025aa1-17d6-42f9-b95f-e91da0819d1b"},{"cell_type":"code","source":["#defining the SQL Query\n","query = f'''\n","    select distinct tu.id\n","    , tu.plan_version\n","    , tu.model\n","    , tu.metric\n","    , tu.time_period\n","    , tu.value\n","    , tu.record_valid_start\n","    , tu.record_valid_end\n","    , tu.transaction_typ\n","    , tu.active_record_flg\n","    from targets_update as tu\n","    left join targets as t \n","        on tu.id = t.id\n","    where t.id is null \n","    union all \n","    select distinct tu.id\n","    , tu.plan_version\n","    , tu.model\n","    , tu.metric\n","    , tu.time_period\n","    , tu.value\n","    , date_trunc('second', to_timestamp('{record_update_time}')) record_valid_start\n","    , tu.record_valid_end\n","    , 'U' transaction_typ\n","    , tu.active_record_flg\n","    from targets_update as tu\n","    inner join targets as t \n","        on tu.id = t.id\n","    where tu.value <> t.value\n","    and t.active_record_flg = true\n","'''"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9809675Z","session_start_time":null,"execution_start_time":"2025-02-10T02:00:49.0067751Z","execution_finish_time":"2025-02-10T02:00:49.2530592Z","parent_msg_id":"4da3464d-8881-4a3f-ab85-3b38c8c6a3de"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9dff5e74-3dfb-477c-8aa1-fff603a196eb"},{"cell_type":"markdown","source":["Step 6 - Loading SQL query into table. You need to store in table to avoid changes to date due to spark's Lazy execution. Lazy execution in Spark is a core concept that significantly impacts its performance and efficiency. It essentially means that Spark delays the execution of transformations until an action is triggered.\n","\n","ecause of this, if a DataFrame references underlying data via a SQL Query that is subsequently modified, later references to that same DataFrame will reflect those changes. Storing the desired data in a staging table ensures consistency and prevents unintended data changes."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6b82853a-943f-4d9f-ab23-f2ed13baa1a6"},{"cell_type":"code","source":["#executing the SQL Query from above into a spark dataframe\n","spark_df_changes = spark.sql(query)\n","#writing the spark dataframe to a staging table to avoid spark lazy Execution\n","spark_df_changes.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"targets_staging\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9815765Z","session_start_time":null,"execution_start_time":"2025-02-10T02:00:49.4103788Z","execution_finish_time":"2025-02-10T02:01:12.7385377Z","parent_msg_id":"bcdd306d-4495-4586-a786-71ceb48c9839"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35ebf8f0-1cf9-4aa1-a37a-5be740124fa2"},{"cell_type":"markdown","source":["Step 7 - Merging updates into the new table involves three key actions: updating existing records (setting their end date to the previously stored timestamp and their active record flag to false), inserting the new records, and finally, dropping the staging table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"717e6c6a-9384-46ba-a36c-1a7e19827fe2"},{"cell_type":"code","source":["#SQL Query for merging in the updates\n","query = f'''\n","    merge into targets t\n","    using targets_staging ts\n","        on t.id = ts.id\n","        and ts.transaction_typ = 'U'\n","    when matched then\n","    update set t.record_valid_end = date_trunc('second', to_timestamp('{record_update_time}')),\n","    t.active_record_flg = false;\n","'''\n","spark.sql(query)\n","#SQL Query for inserting in the new records\n","query = '''\n","    insert into targets\n","    select * from targets_staging;\n","'''\n","spark.sql(query)\n","#SQL Query for droping the staging table\n","query = '''\n","    drop table targets_staging;\n","'''\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9821846Z","session_start_time":null,"execution_start_time":"2025-02-10T02:01:12.9044129Z","execution_finish_time":"2025-02-10T02:01:19.2990047Z","parent_msg_id":"7fb8ac64-c5aa-4bea-8f3b-6db601515463"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1039d9a3-49d3-4b94-bea9-bfea6e8fff2a"},{"cell_type":"code","source":["%%sql\n","select *\n","from targets as a \n","where a.id = '61eb66f1-40ec-5eb6-87dc-5a2906281be1'\n","order by record_valid_start desc"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"d310942a-9e38-4672-a7d6-790e2493c9c5","normalized_state":"finished","queued_time":"2025-02-10T02:00:24.9827781Z","session_start_time":null,"execution_start_time":"2025-02-10T02:01:19.4217005Z","execution_finish_time":"2025-02-10T02:01:21.7661423Z","parent_msg_id":"7fe43c14-12ce-4f86-9e90-e2f97544f3fb"},"text/plain":"StatementMeta(, d310942a-9e38-4672-a7d6-790e2493c9c5, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":9,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"plan_version","type":"string","nullable":true,"metadata":{}},{"name":"model","type":"string","nullable":true,"metadata":{}},{"name":"metric","type":"string","nullable":true,"metadata":{}},{"name":"time_period","type":"string","nullable":true,"metadata":{}},{"name":"value","type":"double","nullable":true,"metadata":{}},{"name":"record_valid_start","type":"date","nullable":true,"metadata":{}},{"name":"record_valid_end","type":"date","nullable":true,"metadata":{}},{"name":"transaction_typ","type":"string","nullable":true,"metadata":{}},{"name":"active_record_flg","type":"boolean","nullable":true,"metadata":{}}]},"data":[["61eb66f1-40ec-5eb6-87dc-5a2906281be1","budget","TW2W57500","units sold","202501",10000000,"2025-02-09","2262-04-11","U",true],["61eb66f1-40ec-5eb6-87dc-5a2906281be1","budget","TW2W57500","units sold","202501",150,"1900-01-01","2025-02-09","I",false]]},"text/plain":"<Spark SQL result set with 2 rows and 10 fields>"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"3f9247b4-fcac-4758-ad80-152ab391b9cb"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"warehouse":{},"lakehouse":{"default_lakehouse":"cef7bf33-231c-4ac7-a055-dc6134700a6c","default_lakehouse_name":"Demo","default_lakehouse_workspace_id":"b7f472bb-a634-48fc-aa12-2d92331bd535"}}},"nbformat":4,"nbformat_minor":5}